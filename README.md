![](https://media.geeksforgeeks.org/wp-content/uploads/20210629203724/MachineLearningwithPythonmin.png)

Les comparto una clase de Machine Learning que brindé a una de mis alumnas en python que radica en Japón. Es muy común tener muchas variables predictoras y por ende mucha correlación entre las mismas, lo cual ya sabemos, que desorienta al modelo común de regresión lineal. Sin embargo existen herramientas que nos permiten efectuar la regresión justamente mitigando este efecto de multicolinealidad. En este caso trataremos sobre el PCA (Principal Components Analysis) o ACP (Análisis de Componentes Principales en español), el cual lo aplicaremos conjuntamente con una regresión tipo forward (para lo cual confeccionaremos una función). Asimismo enfrentaremos el modelo anterior contra una regresión lineal de Partial Least Squares y vesus una regresión con penalidades, en este caso Ridge, (hubiera podido ser Lasso), recordemos que las regresiones de Lasso y Ridge también mitigan la multicolinealidad y a pesar que nos dan estimadores insesgados, nos otorgan mayor precisión en lo modelos (mayor precisión que los modelos basados en estimadores de máxima verosimilitud, que si nos otorgan estimadores insesgados y consistentes).